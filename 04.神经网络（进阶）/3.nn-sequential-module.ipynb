{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3.nn-sequential-module.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengleniubi/L_tensor/blob/main/04.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E8%BF%9B%E9%98%B6%EF%BC%89/3.nn-sequential-module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0xTWI9yJ5G-"
      },
      "source": [
        "# 多层神经网络，Sequential 和 Module\n",
        "通过前面的章节，我们了解到了机器学习领域中最常见的两个模型，线性回归模型和 Logistic 回归模型，他们分别是处理机器学习中最常见的两类问题-回归问题和分类问题。\n",
        "\n",
        "下面我们会讲第一个深度学习的模型，多层神经网络。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1CDkUVRqJ5HI"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VaD_0hFTJ5HJ"
      },
      "source": [
        "def plot_decision_boundary(model, x, y):#边界决定\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mggt5SZ8J5HK"
      },
      "source": [
        "这次我们仍然处理一个二分类问题，但是比前面的 logistic 回归更加复杂"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DqKv-3tvJ5HK"
      },
      "source": [
        "np.random.seed(1)\n",
        "m = 400 # 样本数量\n",
        "N = int(m/2) # 每一类的点的个数\n",
        "D = 2 # 维度\n",
        "x = np.zeros((m, D))\n",
        "y = np.zeros((m, 1), dtype='uint8') # label 向量，0 表示红色，1 表示蓝色\n",
        "a = 4\n",
        "\n",
        "for j in range(2):\n",
        "    ix = range(N*j,N*(j+1))\n",
        "    t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
        "    r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
        "    x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "    y[ix] = j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzjBil_aJ5HL"
      },
      "source": [
        "plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh6AqrdjJ5HM"
      },
      "source": [
        "我们可以先尝试用 logistic 回归来解决这个问题"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sm9ICkCHJ5HN"
      },
      "source": [
        "x = torch.from_numpy(x).float()\n",
        "y = torch.from_numpy(y).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lZTzeA0KJ5HN"
      },
      "source": [
        "w = nn.Parameter(torch.randn(2, 1))\n",
        "b = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "optimizer = torch.optim.SGD([w, b], 1e-1)\n",
        "\n",
        "def logistic_regression(x):\n",
        "    return torch.mm(x, w) + b\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrvhYpK6J5HO"
      },
      "source": [
        "for e in range(100):\n",
        "    out = logistic_regression(Variable(x))\n",
        "    loss = criterion(out, Variable(y))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (e + 1) % 20 == 0:\n",
        "        print('epoch: {}, loss: {}'.format(e+1, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WV2KTFpPJ5HO"
      },
      "source": [
        "def plot_logistic(x):\n",
        "    x = Variable(torch.from_numpy(x).float())\n",
        "    out = F.sigmoid(logistic_regression(x))\n",
        "    out = (out > 0.5) * 1\n",
        "    return out.data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4UHKOxpJ5HO"
      },
      "source": [
        "plot_decision_boundary(lambda x: plot_logistic(x), x.numpy(), y.numpy())\n",
        "plt.title('logistic regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sxXQiEWJ5HP"
      },
      "source": [
        "可以看到，logistic 回归并不能很好的区分开这个复杂的数据集，如果你还记得前面的内容，你就知道 logistic 回归是一个线性分类器，这个时候就该我们的神经网络登场了！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xLsK1L4-J5HP"
      },
      "source": [
        "# 定义两层神经网络的参数\n",
        "w1 = nn.Parameter(torch.randn(2, 4) * 0.01) # 隐藏层神经元个数 2\n",
        "b1 = nn.Parameter(torch.zeros(4))\n",
        "print(w1)\n",
        "print(b1)\n",
        "w2 = nn.Parameter(torch.randn(4, 1) * 0.01)\n",
        "b2 = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "# 定义模型\n",
        "def two_network(x):\n",
        "    x1 = torch.mm(x, w1) + b1\n",
        "    x1 = F.tanh(x1) # 使用 PyTorch 自带的 tanh 激活函数\n",
        "    x2 = torch.mm(x1, w2) + b2\n",
        "    return x2\n",
        "\n",
        "optimizer = torch.optim.SGD([w1, w2, b1, b2], 1.)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdWkzHGvJ5HQ"
      },
      "source": [
        "# 我们训练 10000 次\n",
        "for e in range(20000):\n",
        "    out = two_network(Variable(x))\n",
        "    loss = criterion(out, Variable(y))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (e + 1) % 1000 == 0:\n",
        "        print('epoch: {}, loss: {}'.format(e+1, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Mz-A5sSbJ5HQ"
      },
      "source": [
        "def plot_network(x):\n",
        "    x = Variable(torch.from_numpy(x).float())\n",
        "    x1 = torch.mm(x, w1) + b1\n",
        "    x1 = F.tanh(x1)\n",
        "    x2 = torch.mm(x1, w2) + b2\n",
        "    out = F.sigmoid(x2)\n",
        "    out = (out > 0.5) * 1\n",
        "    return out.data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQt_-c9cJ5HQ"
      },
      "source": [
        "plot_decision_boundary(lambda x: plot_network(x), x.numpy(), y.numpy())\n",
        "plt.title('2 layer network')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLTxfpjNJ5HR"
      },
      "source": [
        "可以看到神经网络能够非常好地分类这个复杂的数据，和前面的 logistic 回归相比，神经网络因为有了激活函数的存在，成了一个非线性分类器，所以神经网络分类的边界更加复杂。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVjxZpaCJ5HR"
      },
      "source": [
        "## Sequential 和 Module\n",
        "我们讲了数据处理，模型构建，loss 函数设计等等内容，但是目前为止我们还没有准备好构建一个完整的机器学习系统，一个完整的机器学习系统需要我们不断地读写模型。在现实应用中，一般我们会将模型在本地进行训练，然后保存模型，接着我们会将模型部署到不同的地方进行应用，所以在这节课我们会教大家如何保存 PyTorch 的模型。\n",
        "\n",
        "首先我们会讲一下 PyTorch 中的模块，Sequential 和 Module。\n",
        "\n",
        "对于前面的线性回归模型、 Logistic回归模型和神经网络，我们在构建的时候定义了需要的参数。这对于比较小的模型是可行的，但是对于大的模型，比如100 层的神经网络，这个时候再去手动定义参数就显得非常麻烦，所以 PyTorch 提供了两个模块来帮助我们构建模型，一个是Sequential，一个是 Module。\n",
        "\n",
        "我们下面分别用 Sequential 和 Module 来定义上面的神经网络。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vfdWO3aLJ5HR"
      },
      "source": [
        "# Sequential\n",
        "seq_net = nn.Sequential(\n",
        "    nn.Linear(2, 4), # PyTorch 中的线性层，wx + b\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4, 1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDhiNWPyJ5HR"
      },
      "source": [
        "# 序列模块可以通过索引访问每一层\n",
        "\n",
        "seq_net[0] # 第一层"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxK1F2dKJ5HS"
      },
      "source": [
        "# 打印出第一层的权重\n",
        "\n",
        "w0 = seq_net[0].weight\n",
        "print(w0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wbqUFBodJ5HS"
      },
      "source": [
        "# 通过 parameters 可以取得模型的参数\n",
        "param = seq_net.parameters()\n",
        "\n",
        "# 定义优化器\n",
        "optim = torch.optim.SGD(param, 1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gssRwCJ5HS"
      },
      "source": [
        "# 我们训练 10000 次\n",
        "for e in range(10000):\n",
        "    out = seq_net(Variable(x))\n",
        "    loss = criterion(out, Variable(y))\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if (e + 1) % 1000 == 0:\n",
        "        print('epoch: {}, loss: {}'.format(e+1, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdxvTQQgJ5HT"
      },
      "source": [
        "可以看到，训练 10000 次 loss 比之前的更低，这是因为 PyTorch 自带的模块比我们写的更加稳定，同时也有一些初始化的问题在里面，关于参数初始化，我们会在后面的课程中讲到"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "LeXF90D4J5HT"
      },
      "source": [
        "def plot_seq(x):\n",
        "    out = F.sigmoid(seq_net(Variable(torch.from_numpy(x).float()))).data.numpy()\n",
        "    out = (out > 0.5) * 1\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbk5hZClJ5HT"
      },
      "source": [
        "plot_decision_boundary(lambda x: plot_seq(x), x.numpy(), y.numpy())\n",
        "plt.title('sequential')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOLecPkkJ5HT"
      },
      "source": [
        "最后我们讲一讲如何保存模型，保存模型在 PyTorch 中有两种方式，一种是将模型结构和参数都保存在一起，一种是只将参数保存下来，下面我们一一介绍。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KaqQMhCSJ5HU"
      },
      "source": [
        "# 将参数和模型保存在一起\n",
        "torch.save(seq_net, 'save_seq_net.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBraXLIgJ5HU"
      },
      "source": [
        "上面就是保存模型的方式，`torch.save`里面有两个参数，第一个是要保存的模型，第二个参数是保存的路径，读取模型的方式也非常简单"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j-58Zp-mJ5HU"
      },
      "source": [
        "# 读取保存的模型\n",
        "seq_net1 = torch.load('save_seq_net.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT8AWAN0J5HU"
      },
      "source": [
        "seq_net1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWTV-VSzJ5HU"
      },
      "source": [
        "print(seq_net1[0].weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytBOyCnyJ5HV"
      },
      "source": [
        "我们可以看到我们重新读入了模型，并且将其命名为 seq_net1，并且打印了第一层的参数\n",
        "\n",
        "下面我们看看第二种保存模型的方式，只保存参数而不保存模型结构"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "v-zztEFfJ5HV"
      },
      "source": [
        "# 保存模型参数\n",
        "torch.save(seq_net.state_dict(), 'save_seq_net_params.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0nvaatpJ5HV"
      },
      "source": [
        "通过上面的方式，我们保存了模型的参数，如果要重新读入模型的参数，首先我们需要重新定义一次模型，接着重新读入参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "k0gaGQiXJ5HV"
      },
      "source": [
        "seq_net2 = nn.Sequential(\n",
        "    nn.Linear(2, 4),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4, 1)\n",
        ")\n",
        "\n",
        "seq_net2.load_state_dict(torch.load('save_seq_net_params.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2lTfM5qJ5HV"
      },
      "source": [
        "seq_net2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq5FWbk8J5HW"
      },
      "source": [
        "print(seq_net2[0].weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0OZBhtmJ5HW"
      },
      "source": [
        "通过这种方式我们也重新读入了相同的模型，打印第一层的参数对比，发现和前面的办法是一样"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhpjEEcuJ5HW"
      },
      "source": [
        "有这两种保存和读取模型的方法，我们推荐使用**第二种**，因为第二种可移植性更强"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNAhJgPJ5HW"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfcfWpvkJ5HW"
      },
      "source": [
        "下面我们再用 Module 定义这个模型，下面是使用 Module 的模板\n",
        "\n",
        "```\n",
        "class 网络名字(nn.Module):\n",
        "    def __init__(self, 一些定义的参数):\n",
        "        super(网络名字, self).__init__()\n",
        "        self.layer1 = nn.Linear(num_input, num_hidden)\n",
        "        self.layer2 = nn.Sequential(...)\n",
        "        ...\n",
        "        \n",
        "        定义需要用的网络层\n",
        "        \n",
        "    def forward(self, x): # 定义前向传播\n",
        "        x1 = self.layer1(x)\n",
        "        x2 = self.layer2(x)\n",
        "        x = x1 + x2\n",
        "        ...\n",
        "        return x\n",
        "```\n",
        "\n",
        "注意的是，Module 里面也可以使用 Sequential，同时 Module 非常灵活，具体体现在 forward 中，如何复杂的操作都能直观的在 forward 里面执行\n",
        "\n",
        "下面我们照着模板实现一下上面的神经网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lirC9PeWJ5HX"
      },
      "source": [
        "class module_net(nn.Module):\n",
        "    def __init__(self, num_input, num_hidden, num_output):\n",
        "        super(module_net, self).__init__()\n",
        "        self.layer1 = nn.Linear(num_input, num_hidden)\n",
        "        \n",
        "        self.layer2 = nn.Tanh()\n",
        "        \n",
        "        self.layer3 = nn.Linear(num_hidden, num_output)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xHZM_fTFJ5HX"
      },
      "source": [
        "mo_net = module_net(2, 4, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3v6Uq9OJ5HX"
      },
      "source": [
        "# 访问模型中的某层可以直接通过名字\n",
        "\n",
        "# 第一层\n",
        "l1 = mo_net.layer1\n",
        "print(l1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFx7klrxJ5HX"
      },
      "source": [
        "# 打印出第一层的权重\n",
        "print(l1.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "60UPAVP4J5HX"
      },
      "source": [
        "# 定义优化器\n",
        "optim = torch.optim.SGD(mo_net.parameters(), 1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Vzu9ncJ5HY"
      },
      "source": [
        "# 我们训练 10000 次\n",
        "for e in range(10000):\n",
        "    out = mo_net(Variable(x))\n",
        "    loss = criterion(out, Variable(y))\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if (e + 1) % 1000 == 0:\n",
        "        print('epoch: {}, loss: {}'.format(e+1, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vKxtbSA7J5HY"
      },
      "source": [
        "# 保存模型\n",
        "torch.save(mo_net.state_dict(), 'module_net.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-2hvYtXJ5HY"
      },
      "source": [
        "可以看到我们得到了相同的结果，而且使用 Sequential 和 Module 来定义模型更加方便\n",
        "\n",
        "在这一节中我们还是使用梯度下降法来优化参数，在神经网络中，这种优化方法有一个特别的名字，反向传播算法，下一次课我们会讲一讲什么是反向传播算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOCE0zq8J5HY"
      },
      "source": [
        "**小练习：改变网络的隐藏层神经元数目，或者试试定义一个 5 层甚至更深的模型，增加训练次数，改变学习率，看看结果会怎么样**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkaJhZmyJ5HY"
      },
      "source": [
        "下面举个例子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qOO3BzFTJ5HZ"
      },
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Linear(2, 10),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(10, 100),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(100, 10),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(10, 1)\n",
        ")\n",
        "\n",
        "optim = torch.optim.SGD(net.parameters(), 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK-z4c_BJ5HZ"
      },
      "source": [
        "# 我们训练 20000 次\n",
        "for e in range(50000):\n",
        "    out = net(Variable(x))\n",
        "    loss = criterion(out, Variable(y))\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if (e + 1) % 1000 == 0:\n",
        "        print('epoch: {}, loss: {}'.format(e+1, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETRxfT-nJ5HZ"
      },
      "source": [
        "def plot_net(x):\n",
        "    out = F.sigmoid(net(Variable(torch.from_numpy(x).float()))).data.numpy()\n",
        "    out = (out > 0.5) * 1\n",
        "    return out\n",
        "\n",
        "plot_decision_boundary(lambda x: plot_net(x), x.numpy(), y.numpy())\n",
        "plt.title('sequential')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}